\documentclass{article} % For LaTeX2e
\usepackage{iclr2016_conference,times}
\usepackage{hyperref,algorithm,algpseudocode,array,tabularx,multirow,caption,subcaption,amsfonts,url,verbatim,enumitem,amsmath,graphicx}
\usepackage{url}

\title{Fast Parallel SAME Gibbs Sampling on \\ General Discrete Bayesian Networks}

\author{Daniel Seita, Haoyu Chen \& John Canny \\
Computer Science Division \\
University of California, Berkeley \\
Berkeley, CS 94720, USA \\
\texttt{\{seita,haoyuchen,canny\}@berkeley.edu}
}

% The \author macro works with any number of authors. There are two commands used to separate the
% names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break the lines. Using \AND
% forces a linebreak at that point. So, if \LaTeX{} puts 3 of 4 authors names on the first line, and
% the last on the second line, try using \AND instead of \And before the third author name.
%
% ICLR requires electronic submissions, processed by \url{http://arxiv.org}. See ICLR's website for
% more instructions.
% 
% If your paper is ultimately accepted, the statement {\tt {\textbackslash}iclrfinalcopy} should be
% inserted to adjust the format to the camera ready requirements.
% 
% The format for the submissions is a variant of the NIPS format.  Please read carefully the
% instructions below, and follow them faithfully.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

\begin{abstract}
A fundamental task in machine learning and other fields is to perform inference on Bayesian
networks. Since exact inference takes exponential time, it is common to use an approximate inference
algorithm such as Gibbs sampling, but this can still be intractable for graphical models with just a
few hundred discrete random variables. In this paper, we address this problem by presenting our
highly optimized Gibbs sampling implementation, which we believe to be the fastest Gibbs sampler
available. Our Gibbs sampler is GPU-accelerated, heavily parallelized, and replicates data via State
Augmented Marginal Estimation to decrease convergence time while reaching higher quality parameter
estimates. Experiments on both synthetic and real data show that our Gibbs sampler is orders of
magnitude faster than JAGS without sacrificing accuracy. Our ultimate objective is to introduce the
Gibbs sampler to researchers in many fields to expand their range of feasible machine learning
problems.
\end{abstract}




\section{Introduction}\label{sec:intro}

In many machine learning applications, one has a distribution $P(X,Z \mid \Theta)$ where $X$ is
observed data, $Z$ is hidden (latent) data, and $\Theta$ are the model parameters. The goal is
generally to find an optimal $\Theta$ with respect to $X$, while marginalizing out $Z$. To represent
these problems, it is common to use graphical models, which combine probability theory and graph
theory to present a robust formalism for probabilistic inference. A Bayesian network is a graphical
model defined by a directed acyclic graph and a set of conditional probability tables (CPTs). Each
CPT represents a local probability distribution $\Pr(X_i \mid X_{\pi_i})$ where $X_i$ is a random
variable, and $X_{\pi_i}$ represents its set of parent nodes (variables) in the graph. We denote the
full set of CPTs as $\Theta$.

In this paper, our focus is on the problem of estimating the parameters of Bayesian networks with
discrete random variables based on partially observed data $\mathcal{D} = \{\xi_1, \ldots, \xi_m\}$,
where $\xi_i$ is an $n$-dimensional vector with assignments to the $n$ variables of the graph, or
``N/A'' to indicate missing data. We assume that the structure of our Bayesian network --- its nodes
and edges --- is known in advance. This type of problem often arises in practice because it is
easier to elicit graph structure from human experts than it is to get numerical
parameters~\citep{Koller2009}. Furthermore, it is often unrealistic to expect our data $\mathcal{D}$
to be completely observed, as data might be missing due to errors (e.g., human oversights) or
deliberate omissions.

Well-known strategies for parameter estimation with partially observed data include
Expectation-Maximization~\citep{EMpaper} and variations of gradient ascent~\citep{Thiesson95}.
Parameter estimation using these methods requires running probabilistic inference over the missing
data, which tends to be the limiting factor since  exact inference using the junction tree algorithm
takes exponential time. It is therefore common to use approximate inference procedures.  One way is
to perform Markov Chain Monte Carlo (MCMC) simulation, where one constructs a Markov chain whose
state is the assignment to all unobserved variables, such that the stationary distribution of the
chain is posterior probability over these variables.

Gibbs sampling~\citep{Geman1984} is a special case of MCMC simulation, which at iteration $t$, goes
through each variable and samples from their full conditionals based on the values already sampled:
$\Pr(X_i^{(t)} \mid X_1^{(t)}, \ldots, X_{i-1}^{(t)}, X_{i+1}^{(t-1)}, \ldots, X_n^{(t-1)})$. For
our parameter estimation problem, we will also need to update $\Theta$, which we can do by sampling
from $P(\Theta \mid \mathcal{D})$, the posterior distribution over the complete data $\mathcal{D}$
(``complete'' due to sampling). We combine $\mathcal{D}$ with a Dirichlet prior to sample for the
new $\Theta$.

While Gibbs sampling is commonly used in machine learning, it is perhaps not known primarily for its
speed or scalability \textbf{Daniel: citation needed}. In a recent result,~\citet{SAME2015} showed
that by combining State Augmented Monte Carlo (SAME) sampling~\citep{SAME2002} with Gibbs sampling,
one can get fast, high quality parameter estimates for discrete graphical models, but they only
applied it to two specific types of models that do not have mutual dependencies between discrete
states.

In this paper, we build upon that result by implementing a general purpose SAME Gibbs sampler to
work on discrete Bayesian networks that have dependencies between discrete states. To be precise,
the novelty and aspects of our Gibbs sampler is that

\begin{itemize}[noitemsep]
    \item our sampler uses an adjustable SAME parameter to replicate data, causing the Gibbs
    sampler converge faster and to higher-quality MAP or ML parameter estimates by reducing the
    excess variance introduced from standard Gibbs sampling.
    \item we exploit advances in GPUs to run the samplers via GPUs with lots of parallelization, and
    thus the sampler can scale up to problems with hundreds of random variables.
    \item the sampler is run on only one computer (and designed to maximize throughput), thus
    avoiding the need to work though complicated distributed systems.
    \item it is open source as part of the BIDMach
    library\footnote{\url{https://github.com/BIDData/BIDMach}} and comes with various diagnostic
    tools. Furthermore, the mini-batch updating nature of BIDMach means we could run our Gibbs
    sampler on data too large to fit in RAM.
\end{itemize}

We benchmark our sampler versus JAGS~\citep{JAGS2003} and show that our Gibbs sampler is orders of
magnitude faster. Consequently, in addition to introducing our Gibbs sampler to researchers, we
argue in this paper that Gibbs sampling --- augmented with SAME sampling --- can be competitive with
or superior to the fastest special purpose inference algorithms for Bayesian inference.
\textbf{Daniel: it might be worth re-emphasizing the dual nature of this contribution more often.}





\section{Related Work}\label{sec:related_work}

The problem of Bayesian inference for graphical models is old and well-studied, and for reference,
we refer the reader to textbooks (e.g., see~\citet{Koller2009} for a broad background) and survey
papers (e.g., see~\citet{Wainwright2008} for a discussion on variational inference as an alternative
to MCMC methods). \textbf{Daniel: I would like to have a survey paper that actually compares Gibbs
sampling versus other methods. I think this paragraph should be longer, and the introduction
slightly condensed.}

Gibbs sampling is also relatively old and well-studied, but it has recently been getting more
attention as the research community explores efforts to improve speed and scalability. The Gibbs
sampling research most related to our contribution involves efforts to (1) parallelize Gibbs
sampling and (2) achieve high throughput. By exploiting the conditional independence assumptions of
Bayesian networks, is possible to have an exact, semi-parallel Gibbs sampler that iterates through
color groups of nodes, and within each group, sampling each node independently~\citep{Gonzalez2011},
a strategy that we use in our sampler. It is also possible to relax the sequential nature of the
algorithm and approximate Gibbs sampling with more limited global communication~\citep{Johnson2013}.
The databases community has also been able to contribute to speeding up Gibbs sampling by showing
how improved system design can lead to higher throughput~\citep{Zhang2013}.

Recently, the use of Graphics Processing Units (GPUs) has become essential for developing high
performance software, as exemplified by popular packages such as Theano for evaluating mathematical
expressions with multidimensional arrays~\citep{Theano2012} and CAFFE for neural
networks~\citep{jia2014caffe}.  The Augur probabilistic programming language~\citep{Augur2014},
which generates inference code for Bayesian networks, demonstrates the importance of combining GPU
code with extra parallelism introduced from exploiting conditional independence assumptions.

The result most directly related to our paper is one that shows how the addition of SAME to a
GPU-accelerated Gibbs sampler can be very fast for Latent Dirichlet Allocation and the Chinese
Restaurant Process~\citep{SAME2015}. In that paper, they explored the application of SAME to
graphical model inference on modern hardware, and showed that combining SAME with factored sample
representation (or approximation) gives throughput competitive with the fastest symbolic methods,
but with potentially better quality. We build up on that result by implementing a more
general-purpose Gibbs sampler that can be applied to arbitrary discrete graphical models.





\section{Fast Parallel SAME Gibbs Sampling}\label{sec:same}

\textbf{Daniel: I think we need John to help us out here -- not sure if we can get away without
explaining a little math? We might also need to talk about reducing variance, but doesn't that
follow from sharpening the distribution?}

SAME is based on~\citep{SAME2002}. It means we define a new joint distribution $Q$:

\begin{equation}\label{eq:same}
Q(X,\Theta,Z^{(1)},\ldots,Z^{(m)}) = \prod_{j=1}^m P(X,\Theta,Z^{(j)})
\end{equation}

which models $m$ copies of the distribution tied to the same set of parameters $\Theta$, which in
our case forms the set of Bayesian network CPTs. It is known that this new distribution $Q$ is up to
a constant factor equal to a \emph{power} of the original distribution, so $Q(\Theta \mid X) \propto
(P(\Theta \mid X))^m$. Thus, it has the same optima, including the global optimum, but its peaks are
considerably sharpened. Also, as $m$ increases, SAME approaches
Expectation-Maximization~\citep{EMpaper}.

\textbf{Daniel: Ideally, I think the first three sections should take up the first three full pages
(roughly, but not more). Then section 4 (the implementation) is on page 4 (and includes the
figure/visualization). Pages 5 through 8 are for experimental results and conclusions.}




\section{Implementation of SAME Gibbs Sampling}\label{sec:implementation}

\begin{figure}[t]
\centering
\includegraphics[width=0.8\textwidth]{fig_BIDMach_flow_DRAFT}
\caption{\textbf{Daniel: I think having a short, fat BIDMach visualization would be great, but it needs
to be publication quality. The above is ``too tall.''} This is a visualization of our Gibbs sampler.
It takes in sparse data, replicates it using SAME (with $m=4$ here), samples them (using the same
set of CPTs), then uses those samples (with a prior and the current CPT) to get the new CPT.}
\label{fig:BIDMach}
\end{figure}

Figure~\ref{fig:BIDMach} shows a visualization of how our Gibbs sampler works. It is implemented and
integrated as part of the open-source BIDMach library~\citep{bidmach} for machine learning. Our
Gibbs sampler expects a data matrix, with rows representing variables and columns representing
cases. BIDMach divides data into ``mini-batches'' to which it iterates sequentially to update
parameters. Going through all mini-batches is considered one full pass over the data.

One of our main contributions is introducing a Gibbs sampler augmented with SAME sampling.
Consequently, if $m$ is the SAME parameter, for each mini-batch our sampler forms $m$ copies of the
data.  The known data is kept the same across all copies. Then, it performs Gibbs sampling to fill
in the unknown values in each copy of the mini-batch using the same set of CPTs.  These sampled
results are combined with an adjustable Dirichlet prior and (if desired) the current
CPT\footnote{This would be useful if one wanted to do a moving average update of the CPT. Also,
the use of the Dirichlet prior tends to be more important when the data is sparse, since it
``smooths'' the CPTs.} to form a set of discrete counts that we then sample from to estimate the
updated CPT. To preserve samples across all runs, each time the sampler processes a mini-batch, it
stores the sampled results in memory. Then, during the next \emph{pass} over the data (i.e., after
having gone through all the mini-batches) it starts the Gibbs sampling process from that stored
data. It is possible to skip the first few samples (the standard \emph{burn-in} period) and
also to only update the parameters every $N^{\rm th}$ iteration to reduce correlated samples.

There are many ways in which we optimize the sampler to maximize throughput on one computer and to
avoid excess memory allocation. First, our sampler is GPU accelerated and takes advantage of
parallelism in modern hardware by implementing the sampling process using matrix operations. Since
memory is scarce on GPUs (3-12 GB is typical), we use a matrix caching strategy to reuse memory for
same-sized matrices.  This is also the reason why BIDMach uses mini-batch updates, since the
corresponding matrices would be of the same size, and thus it re-uses their memory
slots\footnote{This does not generally apply to the very last mini-batch, but that last batch would
not cost too much in extra memory, and one can even ignore it if needed.}. The settings we use are
customizable, so one can reduce the mini-batch size to reduce the memory footprint. One needs to do
this step to perform SAME sampling with high $m$, since more replication takes up more memory.

As briefly mentioned in Section~\ref{sec:related_work}, we further parallelize Gibbs sampling in an
exact manner by using chromatic sampling. One property of Bayesian networks is that $u, v \in
\mathcal{V}$ are independent conditioning on a set of variables $\mathcal{C}$ if $\mathcal{C}$
includes at least one variable on every path connecting $u$ and $v$ in $\tilde{\mathcal{G}}$, which
is the \emph{moralized graph} of the network (i.e., the graph formed by connecting parents and
dropping edge orientations). That is, vertex set $\mathcal{C}$ separates the dependency between $u$
and $v$.

Suppose there is a $k$-coloring of $\tilde{\mathcal{G}}$ such that each vertex is assigned one of $k$
colors and adjacent vertices have different colors. Denote $\mathcal{V}_c$ the set of variables
assigned color $c$ where $1 \leq c \leq k$. One can sample sequentially from $\mathcal{V}_1$ to
$\mathcal{V}_k$, where within each color group, it samples all the variables in parallel. This
parallel sampler corresponds exactly to the execution of a sequential scan Gibbs sampler for some
permutation over the variables and will converge to the desired distribution because variables
within one color group are independent to each other given all the other variables. Finding the
optimal coloring of a graph is NP-complete in general, but efficient heuristics for balanced graph
coloring perform well in many problems.





\section{Experiments}\label{sec:experiments}

We test our Gibbs sampler on one synthetic and one real dataset.  We compare our Gibbs sampler with
JAGS~\citep{JAGS2003}, which is the most popular and efficient tool for Bayesian inference. JAGS is
the fairest comparison to our sampler because it uses Gibbs sampling as the primary inference
algorithm. There are alternative ways of estimating CPTs, but most of them (e.g., Bayes Net Toolbox)
lack the support of efficient inference of discrete models.

We evaluate our sampler and JAGS on a PC equipped with a single 4-core CPU (Intel Core i7-3667U)
and a dual-core GPU (Nvidia GTX-690). Only one core of GPU is used in the benchmark. We also use
Intel VTune Amplifier to profile each program and measure the flops performance. VTune is very power
tool: It allows user to attach any running process and monitor the all the hardware instructions
executed, including both X87 legacy floating point operations and SSE operations.  We emphasize that
the use of one computer is deliberate, as it is cheaper and simpler to use as compared to a
computing cluster.

\textbf{Daniel: The above is for stout, I think. We need the updated specs for bitter, and to change
the instructions (these are copied from Huasha Zhao).}

\subsection{Synthetically Generated Student Data}\label{ssec:student_data}

\begin{figure}[t]
  \centering
  \begin{minipage}{.5\textwidth}
    \centering
    %\includegraphics[width=1\textwidth]{fig_kl_div_25_50_perc}
    \caption{Average KL divegence.}
    \label{fig:kl_divergence}
  \end{minipage}\hfill
    \begin{minipage}{.5\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{fig_kl_div_25_50_perc_jags}
    \caption{JAGS version.}
    \label{fig:kl_accuracy}
  \end{minipage}
\end{figure}

\textbf{Daniel: TODO Change the description here since I am removing that 50 variable student DAG
here. Yeah it is not good to have that.}

We use synthetic data generated from a toy Bayesian network to illustrate proof of correctness and
other concepts. Our data is from~\citet{Koller2009}. The goal is to model a student taking a class,
with metrics of skill (Intelligence and SAT score), the class difficulty, and the student's
resulting grade, which subsequently affects the quality of a letter of recommendation.

To generate the data, we performed direct sampling on a topological ordering of the variables with
$X_0 = {\rm Intelligence}$, $X_1 = {\rm Difficulty},$ $X_2 = {\rm SAT}$, $X_3 = {\rm Grade}$, and
$X_4 = {\rm Letter}$. Variable $X_i$ gets sampled according to the values of its parents (if any)
based on the true distribution. We generated one million samples, which means our data is formatted
in a $(5\times 10^6)$-dimensional matrix. Then, we randomly hid 50\% and 75\% of the data to form
two respective datasets. The goal is to use the remaining known data, and to use Gibbs sampling, to
sample the unknown values, and then those determine the next sample of the CPT. The objective is to
estimate the resulting set of CPTs to see if they match the true ones.

There are several metrics of measuring how well our sampler converges. The one we use is the average
Kullback-Liebler divergence $KL_{\rm avg}$. For two distributions $p(x)$ and $q(x)$, the
KL-divergence is $\sum_x p(x) \log(p(x)/q(x))$ where we iterate over $x$ such that $q(x) > 0$. Since
the set of CPTs has multiple probability distributions (e.g., the ``Grade'' variable ``contributes''
four probability distributions), we must consider all of them. In this data, there are eleven CPTs,
so the ``average KL divergence'' metric we use is

% Daniel: NOTE we could put this earlier in section 5, and define it better b/c we also use it in
% section 5.2.
\begin{equation}\label{eq:avg_kl}
KL_{\rm avg} = \frac{1}{11}\sum_{i=1}^{11} p_i(x) \log\left(\frac{p_i(x)}{q_i(x)}\right).
\end{equation}

Notice that for all $i$, $p_i(x)$ represents the corresponding \emph{true} distribution, and
$q_i(x)$ is what our sampler estimates. Our sampler will sample the missing values, and then use
those counts along with a Dirichlet prior to form a new Dirichlet distribution, which we then sample
from to form the next (ideally, improved) estimate of the true set of CPTs. We use a uniform
Dirichlet prior with all ones for simplicity.

Figure~\ref{fig:kl_accuracy} plots the $Kl_{\rm avg}$ for both versions of the student data (on a log
scale). For each version, we use four different SAME parameters, $m = \{1,5,10,50\}$. Our results
show that, intuitively, the sampler can better converge to the true CPT with more known data (50\%
known in our case). In addition, increasing $m$ results in CPT estimates that more closely match the
true CPTs, though there are diminishing returns, as the increase from $m=1$ to $m=2$ results in a
similar performance improvement as an increase from $m=2$ to $m=50$. To put our $KL_{\rm avg}$
metric in perspective, when $m=50$, the sampler is accurate to the true CPTs by around three
significant figures. \textbf{Daniel: this figure needs the results from the 25\% data, and we need to
make it cleaner, but otherwise that is the main idea. This figure needs to show (a) correctness, and
(b) that higher $m$ leads to better convergence for both sparsity levels. Also, we might}

Next, we evaluate how the SAME parameter $m$ affects the overall runtime of the sampler, since more
samples means the algorithm runs longer.  Figure~\ref{fig:kl_time} plots the average KL divergence
(on a log scale) versus the time, measured in seconds, for SAME parameters $m = \{1,5,10,20,50\}$ on
the 25\% known data (the 50\% known data had similar results). Each ``point'' in the graph (i.e.,
the shapes that are connected via edges) represents the $KL_{\rm avg}$ measured at the end of a pass
over the data over one mini-batch. We see that larger $m$ results in longer runtimes per pass, since
the curve for $m=1$ has points closer together as compared to $m=5$, and so on. The curves for some
of the smaller values of $m$ were not improving substantially after the first set of iterations,
which is why they are not extended as far in the graph. Note that the graph for $m=50$ has a
starting point that is after the other curves, indicating that higher $m$ means a larger initial
startup time (as expected). But as the curve proceeds, it only takes about 25 seconds before the
largest $m$ provides a superior time-accuracy tradeoff. Before that, $m=10$ and $m=20$ appear to be
better.

We conclude that for the $m$ values we test, increasing $m$ results in better accuracy per time
after only a few passes through the data, so there is substantial benefit to SAME sampling. (In
Section~\ref{sec:discussion}, we discuss the time-versus-$m$ tradeoff in more detail.)

\begin{figure}[t]
  \centering
  \begin{minipage}{.5\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{fig_kl_time_log}
    \caption{Accuracy versus time.}
    \label{fig:kl_time}
  \end{minipage}\hfill
    \begin{minipage}{.5\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{fig_kl_div_25_50_perc_jags_time}
    \caption{JAGS version.}
    \label{fig:consecutive_cpts}
  \end{minipage}
\end{figure}

Finally, we also test another way of measuring convergence, which is the average KL divergence
applied to CPTs produced by our sampler on consecutive passes over the data. Thus, instead of
comparing with the true probability distributions, we are comparing the distributions our sampler
estimates. These are shown in Figure~\ref{fig:consecutive_cpts}, which indicates that with higher
$m$ values, the algorithm is quicker to converge to a set of parameter estimates (which themselves
are higher quality, based on Figure~\ref{fig:kl_accuracy}).

It should also be noted that we benchmarked this with JAGS, which also obtains the true CPTs. JAGS,
however, cannot run on the entire million of samples in a reasonable amount of time. It took one
hour for JAGS to run on just 10000 variables. \textbf{TODO Yeah, I think we need to have solid
benchmarks here, and I can cut back on some of the other earlier stuff.}.

\subsection{Real Dynamic Learning Maps ``MOOC'' Data}\label{ssec:mooc_data}

We now benchmark our code on a Bayesian network with a nation-wide examination dataset from the
Dynamic Learning Maps (DLM) project. The dataset contains the assessment (correct or not) of 30,000
students' responses to questions from the DLM Alternate Assessment System. There are 4000 students
and 340 unique questions in the pilot experiment, and the overall completion rate of the questions
is only 2.2\% (assessment questions are tailored for each student). Each of the 340 questions is
considered to be derived from a set of 15 basic concepts, and relations between questions and
concepts and within concepts are given in the DAG file, which we will specify later.  Each question
is considered as a observed node in the Bayesian network, with a very high missing value rate, and
each concept is considered as a hidden node which never gets observed. Each node is binary-valued.
The inference task is to learn the parameter of the network on 80\% of the response assessment and
predict on the rest 20\% of the response.

After data preprocessing, we have our data as a $(334 \times 4367)$-dimensional matrix, where there
are 334 variables and 4367 students. The first 15 variables are latent and never observed across all
students, and they form the set of parents for the remaining 319 variables. Only 2.2 percent of the
data is known, so the data is extremely sparse.

We evaluate our sampler using two methods. The first is the consecutive KL divergence difference
from Figure~\ref{fig:consecutive_cpts}. The second is the prediction accuracy to measure the quality
of the model, which we did not use to evaluate the five-variable student data. As before, we also
benchmark our sampler with JAGS.

Figure~\ref{fig:mooc_cpts} shows the consecutive KL divergences across consecutive iterations. We
see that higher $m$ values result in better convergence. \textbf{Daniel: I finished this plot and
have to describe it. I have data from 1000 iterations but we could easily run this longer. We will
unfortunately have to say that $m=10$ is actually worse (the plot will not go under $m=1$), but
perhaps we are getting into too many local maxs?}

%\subsection{Performance and Runtime}
%We first look at the efficiency of each system measured by giga floating point operations per second
%(Gflops). Intel VTune Amplifier is used to measure the flops numbers. As presented in Figure
%\ref{perf}, BIDMach achieves 5 Gflops and 1 Gflops for GPU and CPU respectively. The Gibbs sampler
%is bottlenecked by the calculation of sampling probability vectors which is implemented using SpMV
%operations. Such flops numbers are the hardware limit of SpMV operation. Jags and Infer.net operates
%at much lower flops rates. Note that the y-axis of the figure is in log-scale. The VTune profile
%results also show that Jags spend 70 \% of the runtime on disk IO, which is highly inefficient. We
%also observe that the memory usage of Infer.net is not efficient: on our PC with 8G memory, it
%cannot scale up to 10000 students (with the same statistics as the DLM pilot dataset we use).

Next, we use prediction accuracy. For each node to be predicted, we sample 50 instances for that
node from the learned network and observed values, and then take the majority as the predicted
value. The accuracy is measured as the percentage of corrected predictions.
Figure~\ref{fig:mooc_accuracy} shows the accuracy results.  \textbf{TODO I would like to do this,
and I think this is important. We should be seeing that accuracy goes up to about 66\%.
Unfortunately, the actual data is skewed in a 66\% to 33\% ratio so ... let me figure out if I can
spin that positively.}

\begin{figure}[t]
  \centering
  \begin{minipage}{.5\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{fig_mooc_kl_div.png}
    \caption{KL Divergence, MOOC}
    \label{fig:mooc_cpts}
  \end{minipage}\hfill
    \begin{minipage}{.5\textwidth}
    \centering
    \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
    %\includegraphics[width=1\textwidth]{fig_kl_student_50perc}
    \caption{\textbf{MOOC ``Accuracy''}}
    \label{fig:mooc_accuracy}
  \end{minipage}
\end{figure}

We emphasize that in terms of runtime, BIDMach is easily able to run this data. In all, it takes
about ten seconds to run Gibbs sampling through this data, whereas JAGS takes hours. In fact, the
main thing that is holding our algorithm back is the limited memory of GPUs, which means we need to
shrink the batch size with large $m$. As memory on GPUs becomes cheaper, we will be able to run
Gibbs sampling and perform graphical model inference on larger datasets.

\textbf{Again, we should formalize the JAGS comparisons, but for this data, isn't it just a
comparison of runtime? Do I need a table? Ideally, I would be able to use JAGS to get accuracy...}




\section{Discussion}\label{sec:discussion}

One interesting thing about the experiments is the contribution of SAME to Gibbs sampling.  Table
(???) shows the time performance and gigaflops of BIDMach on various settings. One can see that
increasing $m$ for $m < 50$ results in steady gflops increases but \emph{without} an equivalent
increase in time. Then, once $m$ gets large, SAME ``saturates'' and the runtime increases while the
gflops stalls. This matches the results from~\citep{SAME2015} and argues for the importance of
testing with a variety of $m$ values.

% This table will be for SAME stuff
\begin{table}[t]
\caption{\textbf{Placeholder: use this for testing $m$ vs. gflops and runtime?}}
\label{sample-table}
\begin{center}
\begin{tabular}{ll}
\multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
\\ \hline \\
Dendrite         &Input terminal \\
Axon             &Output terminal \\
Soma             &Cell body (contains cell nucleus) \\
\end{tabular}
\end{center}
\end{table}

\textbf{TODO more detailed discussion of SAME and runtime/gflops. Perhaps also discuss memory usage
(as I do earlier)? Not else sure what to put here, but I'm not sure if we want to increase the
experimental section of this paper yet again with this stuff.}

\section{Conclusions}\label{sec:conclusions}

We conclude that SAME Gibbs sampling using our sampler is much faster than the state of the
art (JAGS) in Gibbs sampling, and also that it is fast enough to apply to data with several hundreds
of variables. We argue that SAME Gibbs sampling should be the method researchers use to perform
inference on such Bayesian networks. Future work will explore the application of our sampler to a
wider class of real-world datasets.


\subsubsection*{Acknowledgments}

We thank Yang Gao, Biye Jiang, and Huasha Zhao for helpful discussions. Daniel Seita is supported by
the Berkeley and National Physical Science Consortium Fellowships.

% TODO Yeah make the references section a *lot* cleaner!
\bibliography{iclr2016_conference}
\bibliographystyle{iclr2016_conference}








%% APPENDIX
\clearpage
\appendix

\textbf{I expect that we will use eight pages for text, plus the ninth page for references. Then the
remaining material (if any) will go here.}

\section{Description of Figures (Will Remove in Actual Paper Submission)}

\textbf{I am putting descriptions of how I generated the figures here. In the actual paper, it
doesn't make sense to have these but I am not sure where else to put it ... it's a lot easier to
just refer right to the figure here.}

For Figure~\ref{fig:kl_divergence}, I ran the student data with one million students and a batch
size of 25,000 for SAME parameters 1, 5, and 10. I ran this on \texttt{stout}. For each data, I ran
it for 100 iterations. Each ``marker'' in the plot represents the AVERAGE over all the $KL_{\rm
avg}$ metrics, for each mini-batch. Since $1000000/25000=40$, that meant each of the 40 mini-batches
had the $KL_{\rm avg}$ metric computed, then I averaged them to get one marker. So indeed, the
x-axis is correct: one marker really does mean one pass over the full data.

If I need to recover all settings, I can check the \texttt{out\_25perc\_01same.txt} and other
similarly-named files since that captures the BIDMach output.

To me I think it might be more interesting to use FEWER data points since we would see more of a
clear curve in the plot (well, it's on a log scale but still...). That is easy and I have the code
base for that in \texttt{iclr2016\_createKLplots.py}, which is what I also used to generate these
matplotlib images.

For Figure~\ref{fig:mooc_cpts}, I describe this in the Figure6 directory on github.




\begin{comment}
% Daniel: I'm leaving this here for now, because this is some extra stuff from Huasha's old write-up
% that we might use.

We benchmark all the systems on fitting a Bayesian network with a nation-wide examination dataset
from the Dynamic Learning Maps (DLM) project. The dataset contains the assessment (correct or not)
of 30,000 students' responses to questions from the DLM Alternate Assessment System. There are 4000
students and 340 unique questions in the pilot experiment ,and the overall completion rate of the
questions is only 2.2 \% (assessment questions are tailored for each student). Each of the 340
questions is considered to be derived from a set of 15 basic concepts, and relations between
questions and concepts and within concepts are given. Each question is considered as a observed node
in the Bayesian network (with very high missing value rate), and each concept is considered as a
hidden node which never gets observed. Each node takes a binary value. The inference task is to
learn the parameter of the network on 80\% of the response assessment and predict on the rest 20\%
of the response. We use the prediction accuracy to measure the quality of the model. 

\subsection{Performance and Runtime}
We first look at the efficiency of each system measured by giga floating point operations per second
(Gflops). Intel VTune Amplifier is used to measure the flops numbers. As presented in Figure
\ref{perf}, BIDMach achieves 5 Gflops and 1 Gflops for GPU and CPU respectively. The Gibbs sampler
is bottlenecked by the calculation of sampling probability vectors which is implemented using SpMV
operations. Such flops numbers are the hardware limit of SpMV operation. Jags and Infer.net operates
at much lower flops rates. Note that the y-axis of the figure is in log-scale. The VTune profile
results also show that Jags spend 70 \% of the runtime on disk IO, which is highly inefficient. We
also observe that the memory usage of Infer.net is not efficient: on our PC with 8G memory, it
cannot scale up to 10000 students (with the same statistics as the DLM pilot dataset we use).

\begin{figure}[h!]
\centering
\includegraphics[scale = 0.7]{perf2.png}
\caption{Performance Comparison} 
\label{perf}
\end{figure}

Figure \ref{runtime2} shows the runtime until convergence for each inference engine. Again, time is
in log-scale. The Gibbs sample approach converges in about 200 iterations, while the EP algorithm
converges in 50 iterations. Infer.net is 3.5x faster than Jags. This is expected as symbolic method
is usually more efficient than sampling approach. BIDMach is 2-3 orders of magnitude faster than the
other systems. 

We can also verify that BIDMach is doing the same amount of work (floating point operations) as Jags
by multiplying the gflops number in Figure \ref{perf} with the run time in Figure \ref{runtime2}.

\begin{figure}[h!]
\centering
\includegraphics[scale = 0.7]{time2.png}
\caption{Runtime Comparison} 
\label{runtime2}
\end{figure}

\subsection{Prediction Accuracy}
For each node to be predicted, we sample 50 instances for that node from the learned network and
observed values, and then take the majority as the predicted value. The accuracy is measured as the
percentage of corrected predictions. A random guess will give an accuracy of 50\%. Figure
\ref{accuracy} shows the predicted accuracy as a function of number of iterations in training. Both
BIDMach and Jags achieves 65-67\% accuracy in around 200 iterations. However, BIDMach has a huge
advantage in terms of speed as shown in Figure \ref{runtime2}.

\begin{figure}[h!]
\centering
\includegraphics[scale = 0.7]{accuracy.png}
\caption{Accuracy Comparison} 
\label{accuracy}
\end{figure}

\end{comment}




\end{document}


%\section{Citations, figures, tables, references}
%\label{others}
%
%These instructions apply to everyone, regardless of the formatter being used.
%
%\subsection{Citations within the text}
%
%Citations within the text should be based on the {\tt natbib} package and include the authors' last
%names and year (with the ``et~al.'' construct for more than two authors). When the authors or the
%publication are included in the sentence, the citation should not be in parenthesis (as in ``See
%\citet{Hinton06} for more information.''). Otherwise, the citation should be in parenthesis (as in
%``Deep learning shows promise to make progress towards AI~\citep{Bengio+chapter2007}.'').
%
%The corresponding references are to be listed in alphabetical order of authors, in the
%\textsc{References} section. As to the format of the references themselves, any style is acceptable
%as long as it is used consistently.
%
%\subsection{Figures}
%
%All artwork must be neat, clean, and legible. Lines should be dark enough for purposes of
%reproduction; art work should not be hand-drawn. The figure number and caption always appear after
%the figure. Place one line space before the figure caption, and one line space after the figure. The
%figure caption is lower case (except for first word and proper nouns); figures are numbered
%consecutively.
%
%Make sure the figure caption does not get separated from the figure.  Leave sufficient space to
%avoid splitting the figure and figure caption.
%
%You may use color figures.  However, it is best for the figure captions and the paper body to make
%sense if the paper is printed either in black/white or in color.
%\begin{figure}[h]
%\begin{center}
%%\framebox[4.0in]{$\;$}
%\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
%\end{center}
%\caption{Sample figure caption.}
%\end{figure}
%
%\subsection{Tables}
%
%All tables must be centered, neat, clean and legible. Do not use hand-drawn tables. The table number
%and title always appear before the table. See Table~\ref{sample-table}.
%
%Place one line space before the table title, one line space after the table title, and one line
%space after the table. The table title must be lower case (except for first word and proper nouns);
%tables are numbered consecutively.
%
%\begin{table}[t]
%\caption{Sample table title}
%\label{sample-table}
%\begin{center}
%\begin{tabular}{ll}
%\multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
%\\ \hline \\
%Dendrite         &Input terminal \\
%Axon             &Output terminal \\
%Soma             &Cell body (contains cell nucleus) \\
%\end{tabular}
%\end{center}
%\end{table}
%
%\section{Final instructions}
%Do not change any aspects of the formatting parameters in the style files.  In particular, do not
%modify the width or length of the rectangle the text should fit into, and do not change font sizes
%(except perhaps in the \textsc{References} section; see below). Please note that pages should be
%numbered.
%
%\section{Preparing PostScript or PDF files}
%
%Please prepare PostScript or PDF files with paper size ``US Letter'', and not, for example, ``A4''.
%The -t letter option on dvips will produce US Letter files.
%
%Consider directly generating PDF files using \verb+pdflatex+ (especially if you are a MiKTeX user).
%PDF figures must be substituted for EPS figures, however.
%
%Otherwise, please generate your PostScript and PDF files with the following commands:
%\begin{verbatim}
%dvips mypaper.dvi -t letter -Ppdf -G0 -o mypaper.ps
%ps2pdf mypaper.ps mypaper.pdf
%\end{verbatim}
%
%\subsection{Margins in LaTeX}
%
%Most of the margin problems come from figures positioned by hand using \verb+\special+ or other
%commands. We suggest using the command \verb+\includegraphics+ from the graphicx package. Always
%specify the figure width as a multiple of the line width as in the example below using .eps graphics
%\begin{verbatim}
%   \usepackage[dvips]{graphicx} ...
%   \includegraphics[width=0.8\linewidth]{myfile.eps}
%\end{verbatim}
%or % Apr 2009 addition
%\begin{verbatim}
%   \usepackage[pdftex]{graphicx} ...
%   \includegraphics[width=0.8\linewidth]{myfile.pdf}
%\end{verbatim}
%for .pdf graphics.  See section 4.4 in the graphics bundle documentation
%    (\url{http://www.ctan.org/tex-archive/macros/latex/required/graphics/grfguide.ps})
%
%A number of width problems arise when LaTeX cannot properly hyphenate a line. Please give LaTeX
%hyphenation hints using the \verb+\-+ command.
%
%\end{document}
