\documentclass{article} % For LaTeX2e
\usepackage{iclr2016_conference,times}
\usepackage{hyperref,algorithm,algpseudocode,array,tabularx,multirow,caption,subcaption,amsfonts,url,verbatim,enumitem,amsmath,graphicx}
\usepackage{url}

\title{Fast Parallel SAME Gibbs Sampling on \\ General Discrete Bayesian Networks}

\author{Daniel Seita, Haoyu Chen \& John Canny \\
Computer Science Division \\
University of California, Berkeley \\
Berkeley, CS 94720, USA \\
\texttt{\{seita,haoyuchen,canny\}@berkeley.edu}
}

% The \author macro works with any number of authors. There are two commands used to separate the
% names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break the lines. Using \AND
% forces a linebreak at that point. So, if \LaTeX{} puts 3 of 4 authors names on the first line, and
% the last on the second line, try using \AND instead of \And before the third author name.
%
% ICLR requires electronic submissions, processed by \url{http://arxiv.org}. See ICLR's website for
% more instructions.
% 
% If your paper is ultimately accepted, the statement {\tt {\textbackslash}iclrfinalcopy} should be
% inserted to adjust the format to the camera ready requirements.
% 
% The format for the submissions is a variant of the NIPS format.  Please read carefully the
% instructions below, and follow them faithfully.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

\begin{abstract}
A fundamental task in machine learning and related fields is to perform inference on Bayesian
networks. Since exact inference takes exponential time, it is common to use an approximate algorithm
such as Gibbs sampling, but this can still be intractable for graphical models with just a few
hundred binary random variables. In this paper, we address this issue by presenting our highly
optimized Gibbs sampler, which we believe is the fastest one publicly available.  Our Gibbs
sampler is GPU-accelerated, heavily parallelized, and replicates data via State Augmented Marginal
Estimation (SAME) to decrease convergence time while reaching higher quality parameter estimates.
Experiments on both synthetic and real data show that our Gibbs sampler is multiple orders of
magnitude faster than the state of the art sampler, JAGS, without sacrificing accuracy. Our ultimate
objective is to introduce the Gibbs sampler to researchers in many fields to expand their range of
feasible inference problems.
\end{abstract}




\section{Introduction}\label{sec:intro}

In many machine learning applications, one has a distribution $P(X,Z \mid \Theta)$ where $X$ is
observed data, $Z$ is hidden (latent) data, and $\Theta$ represents the model parameters. The goal is
generally to find an optimal $\Theta$ with respect to $X$, while marginalizing out $Z$. To represent
these problems, it is common to use graphical models, which combine probability theory and graph
theory to present a robust formalism for probabilistic inference. A Bayesian network is a graphical
model defined by a directed acyclic graph and a set of conditional probability tables (CPTs). Each
CPT represents a local probability distribution $\Pr(X_i \mid X_{\pi_i})$ where $X_i$ is a random
variable, and $X_{\pi_i}$ represents its set of parent nodes in the graph. We denote the full set of
CPTs as $\Theta$.

In this paper, our focus is on parameter estimation of Bayesian networks with discrete random
variables (``discrete Bayesian networks'') based on partially observed data $\mathcal{D} = \{\xi_1,
\ldots, \xi_m\}$, where $\xi_i$ is an $n$-dimensional vector with assignments to the $n$ variables
of the graph, or ``N/A'' to indicate missing data. We assume that the structure of the Bayesian
network --- its nodes and edges --- is known in advance. This type of problem often arises in
practice because it is easier to elicit graph structure from human experts than it is to get
numerical parameters~\citep{Koller2009}. Furthermore, it is often unrealistic to expect our data
$\mathcal{D}$ to be completely observed, as data might be missing due to errors (e.g., human
oversights) or deliberate omissions.

Well-known strategies for parameter estimation with partially observed data include
Expectation-Maximization~\citep{EMpaper} and variations of gradient ascent~\citep{Thiesson95}.
Parameter estimation using these methods requires running probabilistic inference over the missing
data, which tends to be the limiting factor since  exact inference using the junction tree algorithm
takes exponential time. It is therefore common to use approximate inference procedures.  One way is
to perform Markov Chain Monte Carlo (MCMC) simulation, where one constructs a Markov chain whose
states are an assignment to all unobserved variables, such that the stationary distribution of the
chain is the posterior probability over these variables.

Gibbs sampling~\citep{Geman1984} is a special case of MCMC simulation, which at iteration $t$, goes
through each unobserved variable and samples from its full conditional based on the samples from the
current or previous iteration: $X_i^{(t)}\ {\raise.17ex\hbox{$\scriptstyle\sim$}} \
\Pr(X_i^{(t)} \mid X_1^{(t)}, \ldots, X_{i-1}^{(t)}, X_{i+1}^{(t-1)}, \ldots, X_n^{(t-1)})$. For the
parameter estimation problem, one also needs to sample to update $\Theta$ from $P(\Theta
\mid \mathcal{D})$, the posterior distribution over the complete data data $\mathcal{D}$
(``complete'' due to sampling). Since we assume discrete random variables, the samples are
multinomial counts, so for Bayesian estimation, one can impose a set of Dirichlet priors on
$\Theta$ so that the posterior is also a set of Dirichlets.

While Gibbs sampling is commonly used in machine learning, it is a very broad technique that may not
be able to match the performance of special-purpose inference algorithms without extensive
fine-tuning or using complicated variations~\citep{Murphy2012}. \textbf{Daniel: I am not sure if I
am explaining this last sentence well enough. Murphy's book has some explanation, but not much.} In
a recent result,~\citet{SAME2015} showed that by combining the State Augmented Monte Carlo (SAME)
technique~\citep{SAME2002} with Gibbs sampling, one can get fast, high quality parameter estimates
for discrete graphical models, but they only applied it to two specific models that do not have
mutual dependencies between discrete states.  In this paper, we build upon that result by
presenting a SAME Gibbs sampler for general discrete Bayesian networks. To be precise, the
novelty and aspects of our Gibbs sampler is that

\begin{itemize}[noitemsep]
    \item our sampler uses an adjustable SAME parameter to replicate data, causing the Gibbs sampler
    to converge faster to higher-quality MAP or ML parameter estimates. We believe this is due to
    reduction of excess variance from standard Gibbs sampling.
    \item we run our sampler on state of the art GPUs and parallelize it as much as possible. Our
    sampler can ultimately scale up to problems with hundreds of random variables.
    \item the sampler is designed to maximize throughput on only one computer, thus avoiding the
    need to work though complicated distributed systems.
    \item it is open source as part of the BIDMach
    library\footnote{\url{https://github.com/BIDData/BIDMach}} and comes with various diagnostic
    tools. Furthermore, BIDMach's mini-batch updating and matrix caching features mean we could even
    run our Gibbs sampler on data too large to fit in a computer's RAM.
\end{itemize}

We benchmark our sampler versus the state of the art Gibbs sampler, JAGS~\citep{JAGS2003}, and show
that our Gibbs sampler is multiple orders of magnitude faster. Consequently, in addition to
introducing our Gibbs sampler to researchers, we argue in this paper that Gibbs sampling augmented
with SAME can be competitive with or superior to the fastest special purpose inference algorithms
for Bayesian inference. \textbf{Daniel: my main concern is that we are not talking about these
``special purpose inference algorithms.'' We're just presenting a fast SAME Gibbs sampler.  Also, I
don't know a good way to incorporate factor graphs here; if I did, then we can
cite~\citep{Factorie2009}.}





\section{Related Work}\label{sec:related_work}

The problem of Bayesian inference for graphical models is old and well-studied, and for reference,
we refer the reader to textbooks (e.g., see~\citet{Koller2009} for a broad background) and survey
papers (e.g., see~\citet{Wainwright2008} for a discussion on variational inference as an alternative
to MCMC methods). \textbf{Daniel: I would like to have a survey paper that actually compares Gibbs
sampling versus other methods. I think this paragraph should be expanded, while condensing the
introductory section.}

Gibbs sampling is also relatively old and well-studied, but it has recently been getting more
attention as the research community explores efforts to improve speed and scalability. The Gibbs
sampling research most related to our contribution involves efforts to (1) parallelize Gibbs
sampling and (2) achieve high throughput. By exploiting the conditional independence assumptions of
Bayesian networks, is possible to have an exact, semi-parallel Gibbs sampler that iterates through
color groups of nodes, and within each group, sampling each node independently~\citep{Gonzalez2011},
a strategy that we employ in our sampler. It is also possible to relax the sequential nature of the
algorithm and approximate Gibbs sampling by limiting global communication~\citep{Johnson2013}.  The
databases community has also been able to contribute to speeding up Gibbs sampling by showing how
improved system design can lead to higher throughput~\citep{Zhang2013}.

Recently, the use of Graphics Processing Units (GPUs) has become essential for developing high
performance software, as exemplified by popular packages such as Theano for evaluating mathematical
expressions with multidimensional arrays~\citep{Theano2012} and CAFFE for neural
networks~\citep{jia2014caffe}.  The Augur probabilistic programming language~\citep{Augur2014},
which generates inference code for Bayesian networks, demonstrates the importance of combining GPU
code with extra parallelism introduced from exploiting conditional independence assumptions.

The result most directly related to our paper, as briefly mentioned in Section~\ref{sec:intro}, is
one that shows how the addition of SAME to a GPU-accelerated Gibbs sampler can be very fast for
Latent Dirichlet Allocation and the Chinese Restaurant Process~\citep{SAME2015}. In that paper, they
explored the application of SAME to graphical model inference on modern hardware, and showed that
combining SAME with factored sample representation (or approximation) gives throughput competitive
with the fastest symbolic methods, but with potentially better quality. We (non-trivially) extend
this result by implementing a more general-purpose Gibbs sampler that can be applied to arbitrary
discrete graphical models.





\section{Fast Parallel SAME Gibbs Sampling}\label{sec:same}

SAME is a variant of MCMC where one artificially replicates data to create distributions that
concentrate themselves on the global modes~\citep{SAME2002}. It is an efficient way of performing
MAP estimation in high-dimensional spaces when needing to integrate out a large number of variables.
Given a distribution $P(X,Z\mid \Theta)$, to estimate the most likely $\Theta$ based on the data
$(X,Z)$ using SAME, one would define a new joint $Q$:
\begin{equation}\label{eq:same}
Q(X,\Theta,Z^{(1)},\ldots,Z^{(m)}) = \prod_{j=1}^m P(X,\Theta,Z^{(j)})
\end{equation}
which models $m$ copies of the distribution tied to the same set of parameters $\Theta$, which in
our case forms the set of Bayesian network CPTs. This new distribution $Q$ is proportional to a
\emph{power} of the original distribution, so $Q(\Theta \mid X) \propto (P(\Theta \mid X))^m$. Thus,
it has the same optima, including the global optimum, but its peaks are sharpened~\citep{SAME2002}.
Note that as $m$ increases, SAME approaches Expectation-Maximization~\citep{EMpaper} since the
distribution would peak at the value corresponding to the maximum likelihood estimate.

We argue that SAME is beneficial for Gibbs sampling because it helps to reduce excess variance.  It
is important, however, not to set the SAME replication factor $m$ too high, because that might
result in getting trapped in local maxima in the sharpened distribution. An ideal setup might be to
start with a low replication factor and gradually increase it, a technique similar to simulated
annealing for gradient descent, because both involve ``cooling'' the target distribution to sharpen
the peaks.

\textbf{Daniel: I am really confused about what I should include for this section. I feel like this
just repeats a lot of the SAME description. It might be necessary to do that, but do we have an idea
on what we want to discuss?}




\section{Implementation of SAME Gibbs Sampling}\label{sec:implementation}

\begin{figure}[t]
\centering
\includegraphics[width=0.8\textwidth]{fig_BIDMach_flow_DRAFT}
\caption{This visualizes our Gibbs sampler at work. The original data is has been split up into four
mini-batches, with each one having some known data (shaded gray) and unknown data (white). The
Gibbs sampler replicates the data via SAME $m=3$, samples the unknown values for one mini-batch,
then uses the full data to update the CPTs. The resulting samples are stored in memory so that they
can be used as the starting point for the next time this mini-batch appears. \textbf{Daniel: this
figure needs (1) to get the fourth mini-batch aligned, (2) get the thin arrow non-overlapping with
the prior, (3) center some of the text/arrows, and (4) perhaps we should use a different font? Also
there is some repetition in the caption and in the text.}}
\label{fig:BIDMach}
\end{figure}

Our Gibbs sampler is implemented and integrated as part of the open-source BIDMach
library~\citep{bidmach} for machine learning.  Figure~\ref{fig:BIDMach} shows a visualization of how
it works on real data. Our sampler expects a (usually sparse) data matrix, with rows representing
variables and columns representing cases. BIDMach divides data into same-sized ``mini-batches'' and
iterates through them to update parameters. Going through all mini-batches is considered one full
pass over the data.

Our main contribution is introducing a Gibbs sampler augmented with SAME sampling.  Consequently,
if $m$ is the SAME parameter, for each mini-batch our sampler forms $m$ copies of the known data.
Then, it performs Gibbs sampling to fill in the unknown values in each copy of the mini-batch using
the same set of CPTs.  These sampled results are combined with an adjustable Dirichlet prior and (if
desired) the current CPT\footnote{This would be useful if one wanted to do a moving average update
of the CPT. Also, the use of the Dirichlet prior tends to be more important when the data is sparse,
since it ``smooths'' the CPTs.} to form a set of discrete counts that we then sample from to
estimate the updated CPT. To preserve samples across all runs, each time the sampler processes a
mini-batch, it stores the sampled results in memory.  Then, during the next \emph{pass} over the
data (i.e., after having gone through all the batches) it starts the Gibbs sampling process from
that stored data. It is possible to skip the first few samples (the standard \emph{burn-in} period)
and also to only update the parameters every $N^{\rm th}$ iteration to reduce correlated samples.

There are many ways in which we optimize the sampler to maximize throughput on one computer and to
avoid excess memory allocation. First, our sampler is GPU accelerated and takes advantage of
parallelism in modern hardware by implementing the sampling process using matrix operations. Since
memory is scarce on GPUs (3-12 GB is typical), we use a matrix caching strategy to reuse memory for
matrices of the same dimensions. This is why BIDMach works with same-sized mini-batches, since the
batches themselves, along with all other matrices used as part of the sampling computation, are of
fixed size and therefore BIDMach can re-use their memory slots\footnote{This does not generally
apply to the very last mini-batch, but that last batch would not cost too much in extra memory, and
one can even ignore it if needed.}. Even with caching, default mini-batch sizes might be too large,
but since BIDMach is highly customizable, one can reduce the number of instances for each batch
(i.e., columns in the data matrix) to reduce the memory footprint. In fact, this step is often
necessary to use a very high SAME parameter.

As briefly mentioned in Section~\ref{sec:related_work}, we further parallelize Gibbs sampling in an
exact manner by using chromatic sampling. One property of Bayesian networks is that $u, v \in
\mathcal{V}$ are independent conditioning on a set of variables $\mathcal{C}$ if $\mathcal{C}$
includes at least one variable on every path connecting $u$ and $v$ in $\tilde{\mathcal{G}}$, which
is the \emph{moralized graph} of the network (i.e., the graph formed by connecting parents and
dropping edge orientations). That is, vertex set $\mathcal{C}$ separates the dependency between $u$
and $v$.

Suppose there is a $k$-coloring of $\tilde{\mathcal{G}}$ such that each vertex is assigned one of $k$
colors and adjacent vertices have different colors. Denote $\mathcal{V}_c$ the set of variables
assigned color $c$ where $1 \leq c \leq k$. One can sample sequentially from $\mathcal{V}_1$ to
$\mathcal{V}_k$, where within each color group, it samples all the variables in parallel. This
parallel sampler corresponds exactly to the execution of a sequential scan Gibbs sampler for some
permutation over the variables and will converge to the desired distribution because variables
within one color group are independent to each other given all the other variables. Finding the
optimal coloring of a graph is NP-complete in general, but efficient heuristics for balanced graph
coloring perform well in many problems.





\section{Experiments}\label{sec:experiments}

We benchmark our Gibbs sampler on one synthetic and one real dataset. We compare it with
JAGS~\citep{JAGS2003}, which is the most popular and efficient tool for Bayesian inference, and also
uses Gibbs sampling as the primary inference algorithm. There are alternative software packages that
could be used for parameter estimation, such as Bayes Net Toolbox~\citep{bnt2001}, but we could not
find one that substantially outperformed JAGS.

We evaluate our sampler and JAGS on a PC equipped with a single 4-core CPU (Intel Core i7-3667U)
and a dual-core GPU (Nvidia GTX-690). Only one core of GPU is used in the benchmark. We also use
Intel VTune Amplifier to profile each program and measure the flops performance. VTune allows users
to attach any running process and monitor all the hardware instructions executed, including both X87
legacy floating point operations and SSE operations.

\textbf{Daniel: I need to fix the preceding paragraph to have the specs for Bitter. I know the GPU
is a GeForce GTX Titan X, but what about the processor, and the number of cores, and anything else?
Also, should we mention the gflops?}

We emphasize that the use of one computer is deliberate and a strength of our sampler, as it is
cheaper and simpler to run as compared to a computing cluster.


\subsection{Synthetically Generated Student Data}\label{ssec:student_data}

\begin{figure}[t]
  \centering
  \begin{minipage}{.5\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{fig_kldiv_twobatches}
    \caption{The $KL_{\rm avg}$ for BIDMach.}
    \label{fig:kl_bidmach}
  \end{minipage}\hfill
    \begin{minipage}{.5\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{fig_kl_div_25_50_perc_jags}
    \caption{The $KL_{\rm avg}$ for JAGS.}
    \label{fig:kl_jags}
  \end{minipage}
\end{figure}

We first use synthetic data generated from a toy Bayesian network to show an example of correctness
and the benefits of SAME. The network consists of five variables: $X_0 = {\rm Intelligence}$, $X_1 =
{\rm Difficulty},$ $X_2 = {\rm SAT}$, $X_3 = {\rm Grade}$, and $X_4 = {\rm Letter}$. The directed
edges are $\mathcal{E} = \{(X_0, X_2), (X_0, X_3), (X_1,X_3), (X_3,X_4)\}$, where $(X_i,X_j)$ means
an arrow points from $X_i$ to $X_j$.  Variable $X_3$ is ternary, and all others are binary. The goal
with this network is to model a student taking a class, considering ability metrics (Intelligence
and SAT score), the class difficulty, and the student's resulting grade, which subsequently affects
the quality of a letter of recommendation. All this is from Chapter 3 of~\citet{Koller2009}, which
also lists the true set of CPTs.

To generate the data for BIDMach, we listed the variables in a topological ordering and directly
sampled their values, where $X_i$ got sampled according to the values of its parents (if any) based
on the true distributions from~\citet{Koller2009}. For our initial experiments, we generated 50,000
samples, so the data is formatted in a $(5\times 50,000)$-dimensional matrix\footnote{It should be
noted that this is only for the sake of comparison with JAGS. BIDMach can handle millions of
samples, but for JAGS, we limited the number of cases to a reasonable number to facilitate
comparisons.}. Then, we randomly hid 50\% of the elements of the data matrix. The objective is to
use Gibbs sampling to estimate the set of CPTs, and see how close they match the true ones.

There are several metrics to evaluate our sampler. The one we employ in this paper is the average
Kullback-Liebler divergence of all the distributions in the set of CPTs, denoted as $KL_{\rm avg}$.
For two distributions $p(x)$ and $q(x)$, the KL-divergence is $\sum_x p(x) \log(p(x)/q(x))$ where we
iterate over $x$ such that $q(x) > 0$. In the ``student'' data, there are eleven probability
distributions that form the set of CPTs. For instance, $X_4$ ``contributes'' three distributions:
$\Pr(X_4 \mid X_3 = 0), \Pr(X_4 \mid X_3 = 1)$, and $\Pr(X_4 \mid X_3 = 2)$, where $X_3$ (the
parent) is fixed. Considering all the variables means that $KL_{\rm avg} = \frac{1}{11}
\sum_{i=1}^{11} p_i(x) \log(p_i(x)/q_i(x))$ with $q_i$ the distribution our sampler estimates. We do
not work with the full joint distribution $P(X_1,X_2,X_3,X_4,X_5)$ since it is more straightforward
to work with the ``projected'' CPTs, and because with higher dimensional data, computing the full
joint is computationally intractable and almost all entries would have probabilities essentially at
zero.

Figures~\ref{fig:kl_bidmach} and~\ref{fig:kl_jags} plot the $KL_{\rm avg}$ metric for the student
data using BIDMach and JAGS, respectively (note the log scale), with three SAME replication factors.
Our results indicate that the $KL_{\rm avg}$ for BIDMach and JAGS reach roughly the same values,
with a slight advantage to JAGS, though this is in part because of the log scale on the graph. In
practice, the difference between BIDMach and JAGS is indistinguishable to humans, and both versions
are able to get to the true CPTs to within a tolerance of 0.005.  \textbf{Daniel: I need to
formalize this ``tolerance'' and explain what this is like ``in practice'' since that will give more
intuition.  This is in progress.} Both versions converge to the true distributions quickly after
about ten passes. For BIDMach, we used two batches with 25,000 cases each (more batches result in
faster convergence), but initialized the true CPTs randomly, which may have slowed down the initial
convergence.

In addition, we observe that increasing $m$ results in CPT estimates that more closely match the
true CPTs. The red curves for Figures~\ref{fig:kl_bidmach} and~\ref{fig:kl_jags} correspond to
substantially worse $KL_{\rm avg}$ than the respective yellow and blue curves. The increase from
$m=1$ to $m=5$ has a much larger relative improvement than the increase from $m=5$ to $m=10$,
consistent with our observations of diminishing returns.

Next, we evaluate how the SAME parameter $m$ affects the runtime of the sampler, since more samples
means the algorithm necessarily runs longer. Figures~\ref{fig:kl_time_bidmach}
and~\ref{fig:kl_time_jags} plot $KL_{\rm avg}$ versus total runtime (in seconds) for BIDMach and
JAGS, respectively. As before, $KL_{\rm avg}$ is on a log scale.

There are several interesting observations. The first is that the curves for higher $m$ values start
later due to high initialization costs (this is especially problematic for JAGS). This, along with
the relative marginal benefit of increasing $m$ on this data means that SAME may not have a
beneficial time-accuracy tradeoff. BIDMach, on the other had, has faster initialization costs and
increasing $m$ results in faster convergence.

\textbf{Daniel: the previous paragraph needs a lot of revision, in part because
Figures~\ref{fig:kl_time_bidmach} and~\ref{fig:kl_time_jags} are old figures and we need to revise
them to match up to clarify the time-accuracy tradeoff.}

\begin{figure}[t]
  \centering
  \begin{minipage}{.5\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{fig_kl_time_log}
    \caption{Time and $KL_{\rm avg}$ (BIDMach).}
    \label{fig:kl_time_bidmach}
  \end{minipage}\hfill
    \begin{minipage}{.5\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{fig_kl_div_25_50_perc_jags_time}
    \caption{Time and $KL_{\rm avg}$ (JAGS).}
    \label{fig:kl_time_jags}
  \end{minipage}
\end{figure}

\subsection{Real Dynamic Learning Maps ``MOOC'' Data}\label{ssec:mooc_data}

We now benchmark our code on a Bayesian network with a nation-wide examination dataset from the
Dynamic Learning Maps (DLM) project. The dataset contains the assessment (correct or not) of 30,000
students' responses to questions from the DLM Alternate Assessment System. There are 4000 students
and 340 unique questions in the pilot experiment, and the overall completion rate of the questions
is only 2.2\% (assessment questions are tailored for each student). Each of the 340 questions is
considered to be derived from a set of 15 basic concepts, and relations between questions and
concepts and within concepts are given in the DAG file, which we will specify later.  Each question
is considered as a observed node in the Bayesian network, with a very high missing value rate, and
each concept is considered as a hidden node which never gets observed. Each node is binary-valued.
The inference task is to learn the parameter of the network on 80\% of the response assessment and
predict on the rest 20\% of the response.

After data preprocessing, we have our data as a $(334 \times 4367)$-dimensional matrix, where there
are 334 variables and 4367 students. The first 15 variables are latent and never observed across all
students, and they form the set of parents for the remaining 319 variables. Only 2.2 percent of the
data is known, so the data is extremely sparse.

We evaluate our sampler using two methods. The first is the consecutive KL divergence difference
from Figure~\ref{fig:consecutive_cpts}. The second is the prediction accuracy to measure the quality
of the model, which we did not use to evaluate the five-variable student data. As before, we also
benchmark our sampler with JAGS.

Figure~\ref{fig:mooc_cpts} shows the consecutive KL divergences across consecutive iterations. We
see that higher $m$ values result in better convergence. \textbf{Daniel: I finished this plot and
have to describe it. I have data from 1000 iterations but we could easily run this longer. We will
unfortunately have to say that $m=10$ is actually worse (the plot will not go under $m=1$), but
perhaps we are getting into too many local maxs?}

%\subsection{Performance and Runtime}
%We first look at the efficiency of each system measured by giga floating point operations per second
%(Gflops). Intel VTune Amplifier is used to measure the flops numbers. As presented in Figure
%\ref{perf}, BIDMach achieves 5 Gflops and 1 Gflops for GPU and CPU respectively. The Gibbs sampler
%is bottlenecked by the calculation of sampling probability vectors which is implemented using SpMV
%operations. Such flops numbers are the hardware limit of SpMV operation. Jags and Infer.net operates
%at much lower flops rates. Note that the y-axis of the figure is in log-scale. The VTune profile
%results also show that Jags spend 70 \% of the runtime on disk IO, which is highly inefficient. We
%also observe that the memory usage of Infer.net is not efficient: on our PC with 8G memory, it
%cannot scale up to 10000 students (with the same statistics as the DLM pilot dataset we use).

Next, we use prediction accuracy. For each node to be predicted, we sample 50 instances for that
node from the learned network and observed values, and then take the majority as the predicted
value. The accuracy is measured as the percentage of corrected predictions.
Figure~\ref{fig:mooc_accuracy} shows the accuracy results.  \textbf{TODO I would like to do this,
and I think this is important. We should be seeing that accuracy goes up to about 66\%.
Unfortunately, the actual data is skewed in a 66\% to 33\% ratio so ... let me figure out if I can
spin that positively.}

\begin{figure}[t]
  \centering
  \begin{minipage}{.5\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{fig_mooc_kl_div.png}
    \caption{KL Divergence, MOOC. \textbf{Daniel: Don't worry, I will fix the y-axis labels.}}
    \label{fig:mooc_cpts}
  \end{minipage}\hfill
    \begin{minipage}{.5\textwidth}
    \centering
    \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
    %\includegraphics[width=1\textwidth]{fig_kl_student_50perc}
    \caption{\textbf{Daniel: in progress!}}
    \label{fig:mooc_accuracy}
  \end{minipage}
\end{figure}

We emphasize that in terms of runtime, BIDMach is easily able to run this data. In all, it takes
about ten seconds to run Gibbs sampling through this data, whereas JAGS takes hours. In fact, the
main thing that is holding our algorithm back is the limited memory of GPUs, which means we need to
shrink the batch size with large $m$. As memory on GPUs becomes cheaper, we will be able to run
Gibbs sampling and perform graphical model inference on larger datasets.

\textbf{Again, we should formalize the JAGS comparisons, but for this data, isn't it just a
comparison of runtime? Do I need a table? Ideally, I would be able to use JAGS to get accuracy...}

\subsection{Runtime vs. Throughput Tradeoff}\label{ssec:tradeoff}

One interesting thing about the experiments is the contribution of SAME to Gibbs sampling.
Table~\ref{tab:tradeoff} shows the time performance and gigaflops of BIDMach on various settings.
One can see that increasing $m$ for $m < 50$ results in steady gflops increases but \emph{without}
an equivalent increase in time. Then, once $m$ gets large, SAME ``saturates'' and the runtime
increases while the gflops stalls. This reinforces the conclusions from~\citep{SAME2015} and argues
for the importance of testing with a variety of $m$ values.


% This table will be for SAME stuff
\begin{table}[t]
\caption{The tradeoff of total runtime (in seconds) versus the steady-state throughput (in gflops)
for different $m$ values, on the student data with one million instances. For each trial, we ran 200
iterations of Gibbs sampling, with a batch size of 50,000. As discussed in the text, once $m$ gets
too large, it saturates the sampler and fails to noticeably improve throughput while causing a
proportionally greater increase in runtime. This seems to happen here between $m=30$ and $m=40$.}
\label{tab:tradeoff}
\begin{center}
\begin{tabular}{ |c|c|c|c|c|c|c|c| } 
\hline
          & $m=1$ & $m=5$  & $m=10$ & $m=20$ & $m=30$  & $m=40$  & $m=50$  \\
\hline \hline
GigaFlops & 2.44  & 7.55   & 10.06  & 12.31  & 13.36  & 13.83  & 13.90   \\ 
Runtime   & 66.96 & 107.96 & 162.02 & 265.02 & 366.06 & 471.51 & 586.58  \\
\hline
\end{tabular}
\end{center}
\end{table}

\textbf{TODO more detailed discussion of SAME and runtime/gflops. Perhaps also discuss memory usage
(as I do earlier)? Not else sure what to put here, but I'm not sure if we want to increase the
experimental section of this paper yet again with this stuff.}

Experiment here: 1 million data (full), with 50\% known. Run on bitter, with a batch size of 50,000.

\textbf{Daniel: I did this experiment using the GPU version of the generic gamma random code, which
does not have as high accuracy. I can repeat this experiment with the CPU version, but that might
distort the runtime due to CPU-GPU switching. Any thoughts?}



\section{Conclusions}\label{sec:conclusions}

We conclude that SAME Gibbs sampling using our sampler is much faster than the state of the
art (JAGS) in Gibbs sampling, and also that it is fast enough to apply to data with several hundreds
of variables. We argue that SAME Gibbs sampling should be the method researchers use to perform
inference on such Bayesian networks. Future work will explore the application of our sampler to a
wider class of real-world datasets.


\subsubsection*{Acknowledgments}

We thank Yang Gao, Biye Jiang, and Huasha Zhao for helpful discussions. Daniel Seita is supported by
the Berkeley and National Physical Science Consortium Fellowships.

% TODO Yeah make the references section a *lot* cleaner!
\bibliography{iclr2016_conference}
\bibliographystyle{iclr2016_conference}








%% APPENDIX
\clearpage
\appendix

\textbf{I expect that we will use eight pages for text, plus the ninth page for references. Then the
remaining material (if any) will go here.}

\section{Description of Figures (Will Remove in Actual Paper Submission)}

\textbf{I am putting descriptions of how I generated the figures here. In the actual paper, it
doesn't make sense to have these but I am not sure where else to put it ... it's a lot easier to
just refer right to the figure here.}

For Figure~\ref{fig:kl_divergence}, I ran the student data with one million students and a batch
size of 25,000 for SAME parameters 1, 5, and 10. I ran this on \texttt{stout}. For each data, I ran
it for 100 iterations. Each ``marker'' in the plot represents the AVERAGE over all the $KL_{\rm
avg}$ metrics, for each mini-batch. Since $1000000/25000=40$, that meant each of the 40 mini-batches
had the $KL_{\rm avg}$ metric computed, then I averaged them to get one marker. So indeed, the
x-axis is correct: one marker really does mean one pass over the full data.

If I need to recover all settings, I can check the \texttt{out\_25perc\_01same.txt} and other
similarly-named files since that captures the BIDMach output.

To me I think it might be more interesting to use FEWER data points since we would see more of a
clear curve in the plot (well, it's on a log scale but still...). That is easy and I have the code
base for that in \texttt{iclr2016\_createKLplots.py}, which is what I also used to generate these
matplotlib images.

For Figure~\ref{fig:mooc_cpts}, I describe this in the Figure6 directory on github.




\begin{comment}
% Daniel: I'm leaving this here for now, because this is some extra stuff from Huasha's old write-up
% that we might use.

We benchmark all the systems on fitting a Bayesian network with a nation-wide examination dataset
from the Dynamic Learning Maps (DLM) project. The dataset contains the assessment (correct or not)
of 30,000 students' responses to questions from the DLM Alternate Assessment System. There are 4000
students and 340 unique questions in the pilot experiment ,and the overall completion rate of the
questions is only 2.2 \% (assessment questions are tailored for each student). Each of the 340
questions is considered to be derived from a set of 15 basic concepts, and relations between
questions and concepts and within concepts are given. Each question is considered as a observed node
in the Bayesian network (with very high missing value rate), and each concept is considered as a
hidden node which never gets observed. Each node takes a binary value. The inference task is to
learn the parameter of the network on 80\% of the response assessment and predict on the rest 20\%
of the response. We use the prediction accuracy to measure the quality of the model. 

\subsection{Performance and Runtime}
We first look at the efficiency of each system measured by giga floating point operations per second
(Gflops). Intel VTune Amplifier is used to measure the flops numbers. As presented in Figure
\ref{perf}, BIDMach achieves 5 Gflops and 1 Gflops for GPU and CPU respectively. The Gibbs sampler
is bottlenecked by the calculation of sampling probability vectors which is implemented using SpMV
operations. Such flops numbers are the hardware limit of SpMV operation. Jags and Infer.net operates
at much lower flops rates. Note that the y-axis of the figure is in log-scale. The VTune profile
results also show that Jags spend 70 \% of the runtime on disk IO, which is highly inefficient. We
also observe that the memory usage of Infer.net is not efficient: on our PC with 8G memory, it
cannot scale up to 10000 students (with the same statistics as the DLM pilot dataset we use).

\begin{figure}[h!]
\centering
\includegraphics[scale = 0.7]{perf2.png}
\caption{Performance Comparison} 
\label{perf}
\end{figure}

Figure \ref{runtime2} shows the runtime until convergence for each inference engine. Again, time is
in log-scale. The Gibbs sample approach converges in about 200 iterations, while the EP algorithm
converges in 50 iterations. Infer.net is 3.5x faster than Jags. This is expected as symbolic method
is usually more efficient than sampling approach. BIDMach is 2-3 orders of magnitude faster than the
other systems. 

We can also verify that BIDMach is doing the same amount of work (floating point operations) as Jags
by multiplying the gflops number in Figure \ref{perf} with the run time in Figure \ref{runtime2}.

\begin{figure}[h!]
\centering
\includegraphics[scale = 0.7]{time2.png}
\caption{Runtime Comparison} 
\label{runtime2}
\end{figure}

\subsection{Prediction Accuracy}
For each node to be predicted, we sample 50 instances for that node from the learned network and
observed values, and then take the majority as the predicted value. The accuracy is measured as the
percentage of corrected predictions. A random guess will give an accuracy of 50\%. Figure
\ref{accuracy} shows the predicted accuracy as a function of number of iterations in training. Both
BIDMach and Jags achieves 65-67\% accuracy in around 200 iterations. However, BIDMach has a huge
advantage in terms of speed as shown in Figure \ref{runtime2}.

\begin{figure}[h!]
\centering
\includegraphics[scale = 0.7]{accuracy.png}
\caption{Accuracy Comparison} 
\label{accuracy}
\end{figure}

\end{comment}




\end{document}


%\section{Citations, figures, tables, references}
%\label{others}
%
%These instructions apply to everyone, regardless of the formatter being used.
%
%\subsection{Citations within the text}
%
%Citations within the text should be based on the {\tt natbib} package and include the authors' last
%names and year (with the ``et~al.'' construct for more than two authors). When the authors or the
%publication are included in the sentence, the citation should not be in parenthesis (as in ``See
%\citet{Hinton06} for more information.''). Otherwise, the citation should be in parenthesis (as in
%``Deep learning shows promise to make progress towards AI~\citep{Bengio+chapter2007}.'').
%
%The corresponding references are to be listed in alphabetical order of authors, in the
%\textsc{References} section. As to the format of the references themselves, any style is acceptable
%as long as it is used consistently.
%
%\subsection{Figures}
%
%All artwork must be neat, clean, and legible. Lines should be dark enough for purposes of
%reproduction; art work should not be hand-drawn. The figure number and caption always appear after
%the figure. Place one line space before the figure caption, and one line space after the figure. The
%figure caption is lower case (except for first word and proper nouns); figures are numbered
%consecutively.
%
%Make sure the figure caption does not get separated from the figure.  Leave sufficient space to
%avoid splitting the figure and figure caption.
%
%You may use color figures.  However, it is best for the figure captions and the paper body to make
%sense if the paper is printed either in black/white or in color.
%\begin{figure}[h]
%\begin{center}
%%\framebox[4.0in]{$\;$}
%\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
%\end{center}
%\caption{Sample figure caption.}
%\end{figure}
%
%\subsection{Tables}
%
%All tables must be centered, neat, clean and legible. Do not use hand-drawn tables. The table number
%and title always appear before the table. See Table~\ref{sample-table}.
%
%Place one line space before the table title, one line space after the table title, and one line
%space after the table. The table title must be lower case (except for first word and proper nouns);
%tables are numbered consecutively.
%
%\begin{table}[t]
%\caption{Sample table title}
%\label{sample-table}
%\begin{center}
%\begin{tabular}{ll}
%\multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
%\\ \hline \\
%Dendrite         &Input terminal \\
%Axon             &Output terminal \\
%Soma             &Cell body (contains cell nucleus) \\
%\end{tabular}
%\end{center}
%\end{table}
%
%\section{Final instructions}
%Do not change any aspects of the formatting parameters in the style files.  In particular, do not
%modify the width or length of the rectangle the text should fit into, and do not change font sizes
%(except perhaps in the \textsc{References} section; see below). Please note that pages should be
%numbered.
%
%\section{Preparing PostScript or PDF files}
%
%Please prepare PostScript or PDF files with paper size ``US Letter'', and not, for example, ``A4''.
%The -t letter option on dvips will produce US Letter files.
%
%Consider directly generating PDF files using \verb+pdflatex+ (especially if you are a MiKTeX user).
%PDF figures must be substituted for EPS figures, however.
%
%Otherwise, please generate your PostScript and PDF files with the following commands:
%\begin{verbatim}
%dvips mypaper.dvi -t letter -Ppdf -G0 -o mypaper.ps
%ps2pdf mypaper.ps mypaper.pdf
%\end{verbatim}
%
%\subsection{Margins in LaTeX}
%
%Most of the margin problems come from figures positioned by hand using \verb+\special+ or other
%commands. We suggest using the command \verb+\includegraphics+ from the graphicx package. Always
%specify the figure width as a multiple of the line width as in the example below using .eps graphics
%\begin{verbatim}
%   \usepackage[dvips]{graphicx} ...
%   \includegraphics[width=0.8\linewidth]{myfile.eps}
%\end{verbatim}
%or % Apr 2009 addition
%\begin{verbatim}
%   \usepackage[pdftex]{graphicx} ...
%   \includegraphics[width=0.8\linewidth]{myfile.pdf}
%\end{verbatim}
%for .pdf graphics.  See section 4.4 in the graphics bundle documentation
%    (\url{http://www.ctan.org/tex-archive/macros/latex/required/graphics/grfguide.ps})
%
%A number of width problems arise when LaTeX cannot properly hyphenate a line. Please give LaTeX
%hyphenation hints using the \verb+\-+ command.
%
%\end{document}
